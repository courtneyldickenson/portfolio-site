import{_ as s,c as a,o as n,ae as t}from"./chunks/framework.6g521mOR.js";const E=JSON.parse('{"title":"Support Vector Machines & Dual Formulation","description":"","frontmatter":{"sidebar":false},"headers":[],"relativePath":"projects/svm_dual.md","filePath":"projects/svm_dual.md"}'),l={name:"projects/svm_dual.md"};function h(e,i,p,r,k,o){return n(),a("div",null,i[0]||(i[0]=[t(`<h1 id="support-vector-machines-dual-formulation" tabindex="-1">Support Vector Machines &amp; Dual Formulation <a class="header-anchor" href="#support-vector-machines-dual-formulation" aria-label="Permalink to &quot;Support Vector Machines &amp; Dual Formulation&quot;">​</a></h1><p><strong>Course:</strong> CS 4375 - Machine Learning Foundations <strong>Topic:</strong> Support Vector Machines (SVM), Dual Optimization, KKT Conditions <strong>Tools Used:</strong> Python (manual matrix construction, no libraries) — <strong>No scikit-learn used</strong></p><hr><h2 id="project-overview" tabindex="-1">Project Overview <a class="header-anchor" href="#project-overview" aria-label="Permalink to &quot;Project Overview&quot;">​</a></h2><p>This assignment tackled the theory and practice of <strong>hard-margin Support Vector Machines</strong>, emphasizing solving the <strong>dual problem</strong> from first principles. Rather than using built-in solvers or ML libraries, the assignment required full derivation and low-level implementation of the dual optimization problem using matrix algebra.</p><hr><h2 id="key-concepts-demonstrated" tabindex="-1">Key Concepts Demonstrated <a class="header-anchor" href="#key-concepts-demonstrated" aria-label="Permalink to &quot;Key Concepts Demonstrated&quot;">​</a></h2><ul><li>Deriving the Lagrangian for SVM optimization</li><li>Reformulating the primal as a quadratic programming (QP) dual</li><li>Identifying support vectors using KKT conditions</li><li>Computing weight vector and margin from dual variables</li></ul><hr><h2 id="primal-formulation-hard-margin-svm" tabindex="-1">Primal Formulation (Hard Margin SVM) <a class="header-anchor" href="#primal-formulation-hard-margin-svm" aria-label="Permalink to &quot;Primal Formulation (Hard Margin SVM)&quot;">​</a></h2><p>Given linearly separable data \${(x_i, y_i)}$, the primal objective is:</p><p>$\\min_{w, b} \\frac{1}{2} |w|^2 \\quad \\text{subject to } y_i(w^T x_i + b) \\ge 1 \\quad \\forall i$</p><hr><h2 id="dual-formulation-via-lagrangian" tabindex="-1">Dual Formulation via Lagrangian <a class="header-anchor" href="#dual-formulation-via-lagrangian" aria-label="Permalink to &quot;Dual Formulation via Lagrangian&quot;">​</a></h2><p>We introduce Lagrange multipliers $\\alpha_i \\ge 0$, and define:</p><p>$$ \\mathcal{L}(w, b, \\alpha) = \\frac{1}{2} |w|^2 - \\sum_i \\alpha_i (y_i(w^T x_i + b) - 1) $$</p><h3 id="stationarity-conditions" tabindex="-1">Stationarity Conditions: <a class="header-anchor" href="#stationarity-conditions" aria-label="Permalink to &quot;Stationarity Conditions:&quot;">​</a></h3><p>Taking gradients and setting to zero:</p><p>$\\frac{\\partial \\mathcal{L}}{\\partial w} = w - \\sum_i \\alpha_i y_i x_i = 0 \\Rightarrow w = \\sum_i \\alpha_i y_i x_i$ $\\frac{\\partial \\mathcal{L}}{\\partial b} = -\\sum_i \\alpha_i y_i = 0$</p><p>Plugging back, the <strong>dual becomes</strong>:</p><p>$$ \\max_\\alpha \\sum_i \\alpha_i - \\frac{1}{2} \\sum_i \\sum_j \\alpha_i \\alpha_j y_i y_j x_i^T x_j \\ \\text{subject to } \\sum_i \\alpha_i y_i = 0, \\quad \\alpha_i \\ge 0 $$</p><hr><h2 id="code-constructing-the-dual-problem-from-scratch" tabindex="-1">Code: Constructing the Dual Problem (from scratch) <a class="header-anchor" href="#code-constructing-the-dual-problem-from-scratch" aria-label="Permalink to &quot;Code: Constructing the Dual Problem (from scratch)&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> numpy </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Sample toy data (2D, linearly separable)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.array([[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">4</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.array([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">n </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> len</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(y)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Kernel matrix (linear)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">K </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.dot(X, X.T)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">P </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.outer(y, y) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> K</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">q </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">np.ones(n)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Equality constraint: sum(alpha_i y_i) = 0</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">A </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y.reshape(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">b </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.array([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Inequality constraints: alpha_i &gt;= 0</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">G </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">np.eye(n)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">h </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.zeros(n)</span></span></code></pre></div><p><em>Solving the QP was done manually or using symbolic derivation in this homework — no solvers like <code>cvxopt</code> were allowed.</em></p><hr><h2 id="kkt-conditions-identifying-support-vectors" tabindex="-1">KKT Conditions: Identifying Support Vectors <a class="header-anchor" href="#kkt-conditions-identifying-support-vectors" aria-label="Permalink to &quot;KKT Conditions: Identifying Support Vectors&quot;">​</a></h2><p>Using the optimal $\\alpha^*$, the support vectors are those for which $\\alpha_i &gt; 0$. Once we identify these:</p><ul><li>Compute $w = \\sum_i \\alpha_i y_i x_i$</li><li>Choose any support vector $(x_i, y_i)$ to solve for $b$: $b = y_i - w^T x_i$</li></ul><hr><h2 id="visualizing-the-svm-result" tabindex="-1">Visualizing the SVM Result <a class="header-anchor" href="#visualizing-the-svm-result" aria-label="Permalink to &quot;Visualizing the SVM Result&quot;">​</a></h2><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> plot_svm_boundary</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(X, y, w, b):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    plt.scatter(X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], X[:, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">c</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    ax </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> plt.gca()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    x_vals </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.linspace(np.min(X[:,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, np.max(X[:,</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">100</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    y_vals </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(w[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x_vals </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> b) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> w[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    plt.plot(x_vals, y_vals, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">label</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;SVM Boundary&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    plt.legend()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    plt.grid(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    plt.show()</span></span></code></pre></div><hr><h2 id="reflections" tabindex="-1">Reflections <a class="header-anchor" href="#reflections" aria-label="Permalink to &quot;Reflections&quot;">​</a></h2><p>This project required rigorous symbolic derivation and matrix construction, ensuring that every line of the dual optimization was both mathematically grounded and implemented without high-level abstractions. It strengthened my understanding of optimization theory and its role in modern ML classification.</p><hr>`,36)]))}const g=s(l,[["render",h]]);export{E as __pageData,g as default};
