import{_ as i,c as a,o as n,ae as t}from"./chunks/framework.JvnCEULl.js";const g=JSON.parse('{"title":"Custom ML: SVM, PCA, Boosting","description":"","frontmatter":{"sidebar":false},"headers":[],"relativePath":"projects/Custom_ML.md","filePath":"projects/Custom_ML.md"}'),e={name:"projects/Custom_ML.md"};function l(h,s,r,p,k,o){return n(),a("div",null,s[0]||(s[0]=[t(`<h1 id="custom-ml-svm-pca-boosting" tabindex="-1">Custom ML: SVM, PCA, Boosting <a class="header-anchor" href="#custom-ml-svm-pca-boosting" aria-label="Permalink to &quot;Custom ML: SVM, PCA, Boosting&quot;">​</a></h1><p><strong>Role:</strong> Machine Learning Engineer, Algorithm Designer<br><strong>Tools:</strong> Python, NumPy, Matplotlib</p><hr><h2 id="project-overview" tabindex="-1">Project Overview <a class="header-anchor" href="#project-overview" aria-label="Permalink to &quot;Project Overview&quot;">​</a></h2><p>This project involved implementing several foundational machine learning models — <strong>Support Vector Machines (SVM)</strong>, <strong>Principal Component Analysis (PCA)</strong>, and <strong>Boosting</strong> — entirely from scratch. The work was part of a rigorous academic deep-dive into ML fundamentals, with a strict rule: no use of libraries like <code>scikit-learn</code>. Everything from gradient descent to matrix decomposition was hand-coded and debugged.</p><h3 id="key-focus-areas" tabindex="-1">Key Focus Areas: <a class="header-anchor" href="#key-focus-areas" aria-label="Permalink to &quot;Key Focus Areas:&quot;">​</a></h3><ul><li><strong>Mathematical Implementation</strong> – Translated ML theory directly into working Python code.</li><li><strong>From-Scratch Optimization</strong> – Implemented gradient updates, dual formulations, and projection steps manually.</li><li><strong>Model Evaluation and Visualization</strong> – Used custom metrics and plots to evaluate performance.</li></ul><p>![Custom ML Overview]</p><hr><h2 id="key-contributions" tabindex="-1">Key Contributions <a class="header-anchor" href="#key-contributions" aria-label="Permalink to &quot;Key Contributions&quot;">​</a></h2><ul><li><strong>Coded a working soft-margin SVM</strong> with support for kernel extensions and slack variables.</li><li><strong>Implemented PCA</strong> via eigen decomposition for dimensionality reduction and visualization.</li><li><strong>Created an ensemble boosting system</strong> using exponential loss and weak learners.</li><li><strong>Wrote custom plotting scripts</strong> to visualize decision boundaries and transformed spaces.</li></ul><hr><h2 id="code-highlights" tabindex="-1">Code Highlights <a class="header-anchor" href="#code-highlights" aria-label="Permalink to &quot;Code Highlights&quot;">​</a></h2><h3 id="_1-soft-margin-svm-primal-form" tabindex="-1">1. Soft-Margin SVM (Primal Form) <a class="header-anchor" href="#_1-soft-margin-svm-primal-form" aria-label="Permalink to &quot;1. Soft-Margin SVM (Primal Form)&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> hinge_loss_gradient</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(w, X, y, C):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    margins </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> w)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    grad </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> w </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> C </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (X.T </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (margins </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)))</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> grad</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Gradient descent loop</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> _ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(epochs):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    grad </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> hinge_loss_gradient(w, X, y, C)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    w </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> learning_rate </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> grad</span></span></code></pre></div><h3 id="_2-pca-with-eigen-decomposition" tabindex="-1">2. PCA with Eigen Decomposition <a class="header-anchor" href="#_2-pca-with-eigen-decomposition" aria-label="Permalink to &quot;2. PCA with Eigen Decomposition&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Center and compute covariance</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_centered </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.mean(X, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">axis</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">cov </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.cov(X_centered, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">rowvar</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Eigenvectors sorted by eigenvalues</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">eigvals, eigvecs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.linalg.eigh(cov)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">idx </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.argsort(eigvals)[::</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">principal_components </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> eigvecs[:, idx[:k]]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">X_pca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> X_centered </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> principal_components</span></span></code></pre></div><h3 id="_3-exponential-loss-boosting" tabindex="-1">3. Exponential Loss Boosting <a class="header-anchor" href="#_3-exponential-loss-boosting" aria-label="Permalink to &quot;3. Exponential Loss Boosting&quot;">​</a></h3><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> exponential_loss</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(y_true, y_pred):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.exp(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">y_true </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y_pred).mean()</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Weighted training and model update loop</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(num_iterations):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    clf </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> train_weak_learner(X, y, sample_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    pred </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> clf.predict(X)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    error </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (sample_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (pred </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">!=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y)).sum()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    alpha </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> *</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.log((</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> -</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> error) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (error </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 1e-10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    sample_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> np.exp(</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">alpha </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> y </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> pred)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    sample_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sample_weights.sum()</span></span></code></pre></div><hr><h2 id="results" tabindex="-1">Results <a class="header-anchor" href="#results" aria-label="Permalink to &quot;Results&quot;">​</a></h2><ul><li>Achieved <strong>competitive accuracy</strong> with hand-built models on benchmark datasets.</li><li>Plotted <strong>decision surfaces, loss curves, and dimensional projections</strong> with Matplotlib.</li><li>Gained deep understanding of <strong>optimization logic and model assumptions</strong>.</li></ul><hr><h2 id="challenges-and-solutions" tabindex="-1">Challenges and Solutions <a class="header-anchor" href="#challenges-and-solutions" aria-label="Permalink to &quot;Challenges and Solutions&quot;">​</a></h2><h3 id="_1-numerical-instability-in-pca" tabindex="-1">1. Numerical Instability in PCA <a class="header-anchor" href="#_1-numerical-instability-in-pca" aria-label="Permalink to &quot;1. Numerical Instability in PCA&quot;">​</a></h3><ul><li><strong>Issue:</strong> Small eigenvalues led to noisy projections.</li><li><strong>Fix:</strong> Used centered data and verified eigenvector orthogonality.</li></ul><h3 id="_2-svm-optimization-divergence" tabindex="-1">2. SVM Optimization Divergence <a class="header-anchor" href="#_2-svm-optimization-divergence" aria-label="Permalink to &quot;2. SVM Optimization Divergence&quot;">​</a></h3><ul><li><strong>Issue:</strong> Gradient exploded when learning rate was too high.</li><li><strong>Fix:</strong> Tuned learning rate + added soft margin slack terms.</li></ul><h3 id="_3-boosting-overfitting" tabindex="-1">3. Boosting Overfitting <a class="header-anchor" href="#_3-boosting-overfitting" aria-label="Permalink to &quot;3. Boosting Overfitting&quot;">​</a></h3><ul><li><strong>Issue:</strong> Training loss decreased while test accuracy dropped.</li><li><strong>Fix:</strong> Added early stopping based on validation performance.</li></ul><hr><h2 id="future-enhancements" tabindex="-1">Future Enhancements <a class="header-anchor" href="#future-enhancements" aria-label="Permalink to &quot;Future Enhancements&quot;">​</a></h2><ul><li>Add visualization of margin width and support vectors.</li><li>Implement kernel PCA and RBF SVM for nonlinear feature spaces.</li><li>Refactor into reusable class-based modules for educational use.</li></ul><hr><h2 id="key-takeaways" tabindex="-1">Key Takeaways <a class="header-anchor" href="#key-takeaways" aria-label="Permalink to &quot;Key Takeaways&quot;">​</a></h2><ul><li>Implementing ML from scratch helps reinforce <strong>math intuition</strong> behind models.</li><li>Visualization is crucial for interpreting model behavior and debugging.</li><li>Boosting techniques can outperform standalone models — if tuned carefully.</li></ul><hr>`,37)]))}const E=i(e,[["render",l]]);export{g as __pageData,E as default};
