import{_ as i,c as a,o as e,ae as n}from"./chunks/framework.6g521mOR.js";const c=JSON.parse('{"title":"WeHack 2024 â€“ Custom LLM Chatbot with LangChain","description":"","frontmatter":{"sidebar":false},"headers":[],"relativePath":"projects/We_hack.md","filePath":"projects/We_hack.md"}'),t={name:"projects/We_hack.md"};function l(h,s,r,o,p,k){return e(),a("div",null,s[0]||(s[0]=[n(`<h1 id="wehack-2024-â€“-custom-llm-chatbot-with-langchain" tabindex="-1">WeHack 2024 â€“ Custom LLM Chatbot with LangChain <a class="header-anchor" href="#wehack-2024-â€“-custom-llm-chatbot-with-langchain" aria-label="Permalink to &quot;WeHack 2024 â€“ Custom LLM Chatbot with LangChain&quot;">â€‹</a></h1><p><strong>Team Members:</strong> Courtney Dickenson, Tushar Wani, Ariel Ong, Ryan Farley <strong>Role:</strong> Backend Architecture, LangChain Integration, Full Stack Deployment <strong>Tools:</strong> Node.js, LangChain, OpenAI API, Django, Express.js, Vite, TailwindCSS, Docker</p><hr><h2 id="project-overview" tabindex="-1">Project Overview <a class="header-anchor" href="#project-overview" aria-label="Permalink to &quot;Project Overview&quot;">â€‹</a></h2><p>Built in 24 hours at <strong>WeHack 2024</strong>, this project is a <strong>document-aware chatbot</strong> that lets users upload <code>.txt</code> files and ask questions about them. We combined <strong>vector retrieval</strong> and <strong>LLM generation</strong> using <strong>LangChain</strong>, building the full stack from scratch and deploying it in a Dockerized environment.</p><hr><h2 id="stack-highlights" tabindex="-1">Stack Highlights <a class="header-anchor" href="#stack-highlights" aria-label="Permalink to &quot;Stack Highlights&quot;">â€‹</a></h2><ul><li>ğŸ§  <strong>LangChain + GPT-3.5-turbo</strong> for Retrieval-Augmented Generation (RAG)</li><li>âš™ï¸ <strong>Express.js</strong> backend as LLM controller</li><li>ğŸŒ <strong>Django server</strong> for structured view logic and user routing</li><li>ğŸ¨ <strong>Vite + TailwindCSS</strong> frontend UI</li><li>ğŸ³ <strong>Dockerized full stack</strong> with isolated services for hot reload and modular dev</li></ul><hr><h2 id="ğŸ”§-key-features" tabindex="-1">ğŸ”§ Key Features <a class="header-anchor" href="#ğŸ”§-key-features" aria-label="Permalink to &quot;ğŸ”§ Key Features&quot;">â€‹</a></h2><ul><li><p><strong>Contextual Q&amp;A via LangChain</strong> Upload <code>.txt</code> files and get grounded responses with document-based vector retrieval.</p></li><li><p><strong>Dynamic Prompt Engineering</strong> Used <code>ChatPromptTemplate</code> for fine-tuned response shaping.</p></li><li><p><strong>Layered API Separation</strong> Express.js handled LLM routing, Django managed backend logic â€” clean and modular.</p></li><li><p><strong>Fast Frontend UI</strong> Lightweight, responsive chat interface built with Vite + Tailwind.</p></li></ul><hr><h2 id="ğŸ“-system-architecture" tabindex="-1">ğŸ“ System Architecture <a class="header-anchor" href="#ğŸ“-system-architecture" aria-label="Permalink to &quot;ğŸ“ System Architecture&quot;">â€‹</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">â”œâ”€â”€</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> frontend/</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # Vite + Tailwind UI</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">â”œâ”€â”€</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> backend/</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">           # LangChain + Express.js LLM API</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">â”œâ”€â”€</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> server/</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # Django server (views, models, routing)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">â””â”€â”€</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> docker-compose.yml</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # Container orchestration</span></span></code></pre></div><hr><h3 id="ğŸ§ -langchain-pipeline-logic-javascript" tabindex="-1">ğŸ§  LangChain Pipeline Logic (JavaScript) <a class="header-anchor" href="#ğŸ§ -langchain-pipeline-logic-javascript" aria-label="Permalink to &quot;ğŸ§  LangChain Pipeline Logic (JavaScript)&quot;">â€‹</a></h3><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ modelName: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;gpt-3.5-turbo-instruct&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> });</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> loader</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> TextLoader</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./example.txt&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> docs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loader.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">load</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatPromptTemplate.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">fromTemplate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">\`Answer based on:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&lt;context&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{context}</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&lt;/context&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Question: {input}\`</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> chain</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> createStuffDocumentsChain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ llm: model, prompt });</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> embeddings</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAIEmbeddings</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> vectorstore</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MemoryVectorStore.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">fromDocuments</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(docs, embeddings);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> retriever</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vectorstore.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">asRetriever</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> retrievalChain</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> createRetrievalChain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ combineDocsChain: chain, retriever });</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> result</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> retrievalChain.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">invoke</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ input: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;What is LangSmith?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> });</span></span></code></pre></div><hr><h2 id="ğŸ› ï¸-devops-deployment" tabindex="-1">ğŸ› ï¸ DevOps &amp; Deployment <a class="header-anchor" href="#ğŸ› ï¸-devops-deployment" aria-label="Permalink to &quot;ğŸ› ï¸ DevOps &amp; Deployment&quot;">â€‹</a></h2><ul><li><strong>Docker Compose</strong>: Unified services and enabled parallel development</li><li><strong>Live Reload</strong>: Hot reloading set up for Node + Vite workflows</li><li><strong>Port Isolation</strong>: Each service ran on a distinct port with mapped access</li></ul><hr><h2 id="ğŸ§ª-challenges-and-fixes" tabindex="-1">ğŸ§ª Challenges and Fixes <a class="header-anchor" href="#ğŸ§ª-challenges-and-fixes" aria-label="Permalink to &quot;ğŸ§ª Challenges and Fixes&quot;">â€‹</a></h2><h3 id="ğŸ“š-langchain-documentation-overload" tabindex="-1">ğŸ“š LangChain Documentation Overload <a class="header-anchor" href="#ğŸ“š-langchain-documentation-overload" aria-label="Permalink to &quot;ğŸ“š LangChain Documentation Overload&quot;">â€‹</a></h3><p><strong>Issue:</strong> Constant class migrations and shallow docs (e.g., <code>retrievalChain</code>, <code>StuffDocumentsChain</code>)</p><p><strong>Fix:</strong> Read through LangChain GitHub issues and reverse-engineered working chains from fragmented code samples</p><hr><h3 id="ğŸ”„-async-inconsistencies" tabindex="-1">ğŸ”„ Async Inconsistencies <a class="header-anchor" href="#ğŸ”„-async-inconsistencies" aria-label="Permalink to &quot;ğŸ”„ Async Inconsistencies&quot;">â€‹</a></h3><p><strong>Issue:</strong> Retrieval chain returned <code>undefined</code> despite full pipeline</p><p><strong>Fix:</strong> Refactored async logic, improved error logging, and validated each layer&#39;s return values</p><hr><h3 id="âš ï¸-package-deprecation-whiplash" tabindex="-1">âš ï¸ Package Deprecation Whiplash <a class="header-anchor" href="#âš ï¸-package-deprecation-whiplash" aria-label="Permalink to &quot;âš ï¸ Package Deprecation Whiplash&quot;">â€‹</a></h3><p><strong>Issue:</strong> Classes like <code>ChatOpenAI</code> suddenly moved packages</p><p><strong>Fix:</strong> Switched to <code>@langchain/openai</code> and <code>@langchain/community</code> where stable documentation lived</p><hr><h3 id="â™»ï¸-3am-full-stack-pivot" tabindex="-1">â™»ï¸ 3AM Full Stack Pivot <a class="header-anchor" href="#â™»ï¸-3am-full-stack-pivot" aria-label="Permalink to &quot;â™»ï¸ 3AM Full Stack Pivot&quot;">â€‹</a></h3><p><strong>Issue:</strong> LangChainâ€™s Python stack (with FastAPI) was unstable mid-hack</p><p><strong>Fix:</strong> We pivoted entirely to <strong>Node.js + Express</strong> to reduce cross-stack bugs and save the deadline</p><hr><h2 id="âœ…-final-results" tabindex="-1">âœ… Final Results <a class="header-anchor" href="#âœ…-final-results" aria-label="Permalink to &quot;âœ… Final Results&quot;">â€‹</a></h2><ul><li>ğŸš€ Fully working RAG chatbot MVP in &lt; 24 hours</li><li>ğŸ”— LangChain + OpenAI + custom document processing</li><li>ğŸ“† Dockerized dev setup with real-time hot reload</li><li>ğŸ§  Delivered grounded, usable LLM responses live on demo day</li></ul><hr><h2 id="ğŸ”®-future-plans" tabindex="-1">ğŸ”® Future Plans <a class="header-anchor" href="#ğŸ”®-future-plans" aria-label="Permalink to &quot;ğŸ”® Future Plans&quot;">â€‹</a></h2><ul><li>Support for <strong>PDF, Markdown, CSV ingestion</strong></li><li>Add <strong>persistent vector DB</strong> like Pinecone or Qdrant</li><li>Build out a <strong>multi-user dashboard</strong> with file history</li><li>Add <strong>voice input support</strong> via Web Speech API</li></ul><hr><h2 id="ğŸ’¬-key-takeaways" tabindex="-1">ğŸ’¬ Key Takeaways <a class="header-anchor" href="#ğŸ’¬-key-takeaways" aria-label="Permalink to &quot;ğŸ’¬ Key Takeaways&quot;">â€‹</a></h2><ul><li>We made LangChain work under pressure â€” even mid-migration</li><li>I built and deployed a <strong>multi-stack LLM app</strong> with Dockerized services and integrated frontend/backend logic</li><li>This project taught us to debug fast, deploy clean, and pivot hard when needed â€” and we still crossed the finish line on time</li></ul><hr>`,47)]))}const g=i(t,[["render",l]]);export{c as __pageData,g as default};
