import{_ as i,c as a,o as e,ae as n}from"./chunks/framework.6g521mOR.js";const c=JSON.parse('{"title":"WeHack 2024 – Custom LLM Chatbot with LangChain","description":"","frontmatter":{"sidebar":false},"headers":[],"relativePath":"projects/We_hack.md","filePath":"projects/We_hack.md"}'),t={name:"projects/We_hack.md"};function l(h,s,r,o,p,k){return e(),a("div",null,s[0]||(s[0]=[n(`<h1 id="wehack-2024-–-custom-llm-chatbot-with-langchain" tabindex="-1">WeHack 2024 – Custom LLM Chatbot with LangChain <a class="header-anchor" href="#wehack-2024-–-custom-llm-chatbot-with-langchain" aria-label="Permalink to &quot;WeHack 2024 – Custom LLM Chatbot with LangChain&quot;">​</a></h1><p><strong>Team Members:</strong> Courtney Dickenson, Tushar Wani, Ariel Ong, Ryan Farley <strong>Role:</strong> Backend Architecture, LangChain Integration, Full Stack Deployment <strong>Tools:</strong> Node.js, LangChain, OpenAI API, Django, Express.js, Vite, TailwindCSS, Docker</p><hr><h2 id="project-overview" tabindex="-1">Project Overview <a class="header-anchor" href="#project-overview" aria-label="Permalink to &quot;Project Overview&quot;">​</a></h2><p>Built in 24 hours at <strong>WeHack 2024</strong>, this project is a <strong>document-aware chatbot</strong> that lets users upload <code>.txt</code> files and ask questions about them. We combined <strong>vector retrieval</strong> and <strong>LLM generation</strong> using <strong>LangChain</strong>, building the full stack from scratch and deploying it in a Dockerized environment.</p><hr><h2 id="stack-highlights" tabindex="-1">Stack Highlights <a class="header-anchor" href="#stack-highlights" aria-label="Permalink to &quot;Stack Highlights&quot;">​</a></h2><ul><li>🧠 <strong>LangChain + GPT-3.5-turbo</strong> for Retrieval-Augmented Generation (RAG)</li><li>⚙️ <strong>Express.js</strong> backend as LLM controller</li><li>🌐 <strong>Django server</strong> for structured view logic and user routing</li><li>🎨 <strong>Vite + TailwindCSS</strong> frontend UI</li><li>🐳 <strong>Dockerized full stack</strong> with isolated services for hot reload and modular dev</li></ul><hr><h2 id="🔧-key-features" tabindex="-1">🔧 Key Features <a class="header-anchor" href="#🔧-key-features" aria-label="Permalink to &quot;🔧 Key Features&quot;">​</a></h2><ul><li><p><strong>Contextual Q&amp;A via LangChain</strong> Upload <code>.txt</code> files and get grounded responses with document-based vector retrieval.</p></li><li><p><strong>Dynamic Prompt Engineering</strong> Used <code>ChatPromptTemplate</code> for fine-tuned response shaping.</p></li><li><p><strong>Layered API Separation</strong> Express.js handled LLM routing, Django managed backend logic — clean and modular.</p></li><li><p><strong>Fast Frontend UI</strong> Lightweight, responsive chat interface built with Vite + Tailwind.</p></li></ul><hr><h2 id="📁-system-architecture" tabindex="-1">📁 System Architecture <a class="header-anchor" href="#📁-system-architecture" aria-label="Permalink to &quot;📁 System Architecture&quot;">​</a></h2><div class="language-bash vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">bash</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">├──</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> frontend/</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">          # Vite + Tailwind UI</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">├──</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> backend/</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">           # LangChain + Express.js LLM API</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">├──</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> server/</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # Django server (views, models, routing)</span></span>
<span class="line"><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">└──</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> docker-compose.yml</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> # Container orchestration</span></span></code></pre></div><hr><h3 id="🧠-langchain-pipeline-logic-javascript" tabindex="-1">🧠 LangChain Pipeline Logic (JavaScript) <a class="header-anchor" href="#🧠-langchain-pipeline-logic-javascript" aria-label="Permalink to &quot;🧠 LangChain Pipeline Logic (JavaScript)&quot;">​</a></h3><div class="language-js vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">js</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAI</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ modelName: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;gpt-3.5-turbo-instruct&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> });</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> loader</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> TextLoader</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./example.txt&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> docs</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> loader.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">load</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> prompt</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ChatPromptTemplate.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">fromTemplate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">\`Answer based on:</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&lt;context&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{context}</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&lt;/context&gt;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">Question: {input}\`</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> chain</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> createStuffDocumentsChain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ llm: model, prompt });</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> embeddings</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> new</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> OpenAIEmbeddings</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> vectorstore</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MemoryVectorStore.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">fromDocuments</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(docs, embeddings);</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> retriever</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> vectorstore.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">asRetriever</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">();</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> retrievalChain</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> createRetrievalChain</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ combineDocsChain: chain, retriever });</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">const</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> result</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> =</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;"> await</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> retrievalChain.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">invoke</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">({ input: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;What is LangSmith?&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> });</span></span></code></pre></div><hr><h2 id="🛠️-devops-deployment" tabindex="-1">🛠️ DevOps &amp; Deployment <a class="header-anchor" href="#🛠️-devops-deployment" aria-label="Permalink to &quot;🛠️ DevOps &amp; Deployment&quot;">​</a></h2><ul><li><strong>Docker Compose</strong>: Unified services and enabled parallel development</li><li><strong>Live Reload</strong>: Hot reloading set up for Node + Vite workflows</li><li><strong>Port Isolation</strong>: Each service ran on a distinct port with mapped access</li></ul><hr><h2 id="🧪-challenges-and-fixes" tabindex="-1">🧪 Challenges and Fixes <a class="header-anchor" href="#🧪-challenges-and-fixes" aria-label="Permalink to &quot;🧪 Challenges and Fixes&quot;">​</a></h2><h3 id="📚-langchain-documentation-overload" tabindex="-1">📚 LangChain Documentation Overload <a class="header-anchor" href="#📚-langchain-documentation-overload" aria-label="Permalink to &quot;📚 LangChain Documentation Overload&quot;">​</a></h3><p><strong>Issue:</strong> Constant class migrations and shallow docs (e.g., <code>retrievalChain</code>, <code>StuffDocumentsChain</code>)</p><p><strong>Fix:</strong> Read through LangChain GitHub issues and reverse-engineered working chains from fragmented code samples</p><hr><h3 id="🔄-async-inconsistencies" tabindex="-1">🔄 Async Inconsistencies <a class="header-anchor" href="#🔄-async-inconsistencies" aria-label="Permalink to &quot;🔄 Async Inconsistencies&quot;">​</a></h3><p><strong>Issue:</strong> Retrieval chain returned <code>undefined</code> despite full pipeline</p><p><strong>Fix:</strong> Refactored async logic, improved error logging, and validated each layer&#39;s return values</p><hr><h3 id="⚠️-package-deprecation-whiplash" tabindex="-1">⚠️ Package Deprecation Whiplash <a class="header-anchor" href="#⚠️-package-deprecation-whiplash" aria-label="Permalink to &quot;⚠️ Package Deprecation Whiplash&quot;">​</a></h3><p><strong>Issue:</strong> Classes like <code>ChatOpenAI</code> suddenly moved packages</p><p><strong>Fix:</strong> Switched to <code>@langchain/openai</code> and <code>@langchain/community</code> where stable documentation lived</p><hr><h3 id="♻️-3am-full-stack-pivot" tabindex="-1">♻️ 3AM Full Stack Pivot <a class="header-anchor" href="#♻️-3am-full-stack-pivot" aria-label="Permalink to &quot;♻️ 3AM Full Stack Pivot&quot;">​</a></h3><p><strong>Issue:</strong> LangChain’s Python stack (with FastAPI) was unstable mid-hack</p><p><strong>Fix:</strong> We pivoted entirely to <strong>Node.js + Express</strong> to reduce cross-stack bugs and save the deadline</p><hr><h2 id="✅-final-results" tabindex="-1">✅ Final Results <a class="header-anchor" href="#✅-final-results" aria-label="Permalink to &quot;✅ Final Results&quot;">​</a></h2><ul><li>🚀 Fully working RAG chatbot MVP in &lt; 24 hours</li><li>🔗 LangChain + OpenAI + custom document processing</li><li>📆 Dockerized dev setup with real-time hot reload</li><li>🧠 Delivered grounded, usable LLM responses live on demo day</li></ul><hr><h2 id="🔮-future-plans" tabindex="-1">🔮 Future Plans <a class="header-anchor" href="#🔮-future-plans" aria-label="Permalink to &quot;🔮 Future Plans&quot;">​</a></h2><ul><li>Support for <strong>PDF, Markdown, CSV ingestion</strong></li><li>Add <strong>persistent vector DB</strong> like Pinecone or Qdrant</li><li>Build out a <strong>multi-user dashboard</strong> with file history</li><li>Add <strong>voice input support</strong> via Web Speech API</li></ul><hr><h2 id="💬-key-takeaways" tabindex="-1">💬 Key Takeaways <a class="header-anchor" href="#💬-key-takeaways" aria-label="Permalink to &quot;💬 Key Takeaways&quot;">​</a></h2><ul><li>We made LangChain work under pressure — even mid-migration</li><li>I built and deployed a <strong>multi-stack LLM app</strong> with Dockerized services and integrated frontend/backend logic</li><li>This project taught us to debug fast, deploy clean, and pivot hard when needed — and we still crossed the finish line on time</li></ul><hr>`,47)]))}const g=i(t,[["render",l]]);export{c as __pageData,g as default};
