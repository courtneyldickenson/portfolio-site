---
title: "The Model Has Overfit"
description: "Can you reverse-engineer this ML modelâ€™s logic?"
---

# ğŸ¤– The Model Has Overfit

You've been handed a black-box ML model that was trained on a tiny dataset.
It gives one of three outputs:
- âœ… **YES** (Confident match)
- â“ **MAYBE** (Uncertain prediction)
- âŒ **NO** (Out of distribution)

But here's the thing... the model seems *weird*. Maybe itâ€™s learned the wrong thing?

---

## ğŸ§ª Training Data:

| Input                     | Output |
|--------------------------|--------|
| red triangle, 3 dots     | âœ… YES |
| blue square, 3 dots      | âœ… YES |
| green circle, 3 dots     | âœ… YES |
| red square, 2 dots       | âŒ NO  |
| green triangle, 4 dots   | âŒ NO  |

---

## ğŸ” Predict the Output:
Try to figure out what the model learned! Choose your predictions below.
---
title: The Model Has Overfit
description: Can you reverse-engineer this ML modelâ€™s logic?
---

<script setup>
import OverfitRiddle from './.vitepress/components/OverfitRiddle.vue'
</script>

<ClientOnly>
  <OverfitRiddle />
</ClientOnly>

---

### ğŸ’¡ Hint:
Donâ€™t overthink itâ€¦ or do? ğŸ˜‰

---

### ğŸ§  Solved it?
If your answers match the model's predictions:

```txt
ğŸ‰ You got it! ğŸ‰

The model overfit to a single feature: the number of dots.
It learned to say YES only if there are exactly 3 dots â€” completely ignoring shape and color.

This is what happens when models learn correlations instead of meaning.
```

<style scoped>
table {
  font-size: 1rem;
  margin-bottom: 1.5rem;
}
</style>
